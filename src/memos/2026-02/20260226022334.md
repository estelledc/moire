---
created: 2026-02-26T02:23:34Z
modified: 2026-02-26T02:23:52Z
---

描述你使用了哪些 AI 工具，你用它们来做什么，以及你学到了什么。

开发过程中使用的 AI 工具

Claude Code（Anthropic CLI）——用于辅助编码。我用它来加速 MVVM 架构的搭建、调试 AVAudioEngine 和 Vision 框架的集成问题、以及迭代 UI 组件的实现。它帮助我快速理解 SoundAnalysis 和 FoundationModels 这些我之前没用过的框架 API，并在遇到编译错误时提供修复建议。Git 提交历史中的 Co-Authored-By: Claude 标记记录了这些协作。

Apple Create ML——用于训练通勤噪音分类模型。我用 Create ML 6.2 的 Sound Classifier 模板，基于 Apple 的 AudioFeaturePrint 迁移学习基础，训练了一个能识别三类通勤噪音（crowd 人群、announcement 广播、friction 摩擦声）的轻量级分类器。训练只用了不到一秒——因为 AudioFeaturePrint 已经完成了音频特征提取的重活，我只需要在上面训练一个广义线性模型（GLM）。

应用内集成的 AI 技术

CoreML + SoundAnalysis——实时环境噪音感知。SoundClassifier 模型通过 SNAudioStreamAnalyzer 接收来自 AVAudioEngine 的实时音频流，每帧输出三类噪音的置信度分布。我用这些置信度计算"压力噪音比"（stress ratio），量化环境干扰对专注力的影响。当 ML 管线不可用时（比如没有麦克风权限），系统自动降级为 AVAudioRecorder 的分贝计量模式。

Vision 框架——视觉注意力追踪。通过前置摄像头每 0.4 秒采样一帧，用 VNDetectFaceLandmarksRequest 提取眼部地标和头部姿态。我实现了基于 Eye Aspect Ratio（EAR）的眨眼检测算法（阈值 < 0.22），以及基于 yaw/roll 标准差的头部稳定性评分。这些数据在 30 秒滑动窗口内聚合，生成视觉稳定性指标。

Apple FoundationModels（iOS 26）——设备端 LLM 生成个性化建议。训练结束后，应用将训练时长、噪音干扰百分比、专注力评分、视觉稳定性等指标传入 SystemLanguageModel.default，由设备端大语言模型生成一句个性化的教练建议（60-120 字符）。我设计了"规则优先、LLM 升级"的降级策略：界面先立即显示规则引擎的确定性建议，LLM 结果返回后再替换，并用标签（"Rule-based" / "On-device"）标注来源，让用户知道建议是怎么生成的。

我学到了什么

迁移学习的实用性：Create ML 的 AudioFeaturePrint 让我用极少的训练数据和时间就得到了一个可用的声音分类器。我学到了"不需要从零训练"——利用预训练特征提取器，在上面微调一个简单分类器，对于特定领域的小任务已经足够。

AI 输出需要约束和验证：FoundationModels 的 LLM 输出不可预测，我学到了必须对它做严格的后处理——字符数验证、只取第一句、支持中英文句号分隔、设置 token 上限。同时，永远需要一个确定性的 fallback（规则引擎），因为 LLM 可能不可用、太慢、或输出不合格。

AI 辅助编码改变了学习曲线，但不替代理解：Claude Code 帮我快速上手了 SoundAnalysis 和 Vision 这些陌生框架，但每次它给出的代码我都需要理解为什么这样写——比如 EAR 算法的数学原理、SNAudioStreamAnalyzer 的线程模型、FoundationModels 的条件编译策略。AI 工具加速了"从 0 到能跑"的过程，但"从能跑到理解"仍然需要自己去读文档和调试。
